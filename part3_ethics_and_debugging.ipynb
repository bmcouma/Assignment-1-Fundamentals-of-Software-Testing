#STEP 4: Ethics & Debugging

- *Bias in MNIST*: Dataset may favor users with Western handwriting styles. Could exclude diverse digit styles.
- *Bias in Amazon Reviews**: May reflect social or cultural biases (e.g., targeting female-coded products more harshly).

*Fixes*:
- Use balanced datasets.
- Integrate tools like TensorFlow Fairness Indicators.
- Rule-based filters to flag and mitigate biased keywords (in spaCy).


# Common bug: wrong loss for integer labels
model.compile(optimizer='adam', loss='categorical_crossentropy')  # ❌

# Fix:
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')  # ✅
