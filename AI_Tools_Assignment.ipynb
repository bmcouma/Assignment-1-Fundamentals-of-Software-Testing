{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8qw3tTqAUD9OQWw0vU0UZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bmcouma/Assignment-1-Fundamentals-of-Software-Testing/blob/main/AI_Tools_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eLd22YONHDJ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Part 1: Theoretical Understanding\n",
        "\n",
        "\n",
        "Q1: **Explain the primary differences between TensorFlow and PyTorch. When would you choose one over the other?**\n",
        "\n",
        "| Feature            | TensorFlow                                                       | PyTorch                                                             |\n",
        "| ------------------ | ---------------------------------------------------------------- | ------------------------------------------------------------------- |\n",
        "| **Execution Mode** | Uses static computation graphs (via `tf.function`)               | Uses dynamic computation graphs (eager execution by default)        |\n",
        "| **Syntax**         | More verbose and abstract                                        | More Pythonic and intuitive                                         |\n",
        "| **Visualization**  | Strong visualization tools like TensorBoard                      | Basic support (use TensorBoard with `torch.utils.tensorboard`)      |\n",
        "| **Deployment**     | Easy model export via TensorFlow Serving, TFLite, TensorFlow\\.js | Deployment via TorchScript, ONNX, but slightly less straightforward |\n",
        "| **Ecosystem**      | Well-integrated with Keras, TFX, TPU support                     | Better for rapid prototyping, research work                         |\n",
        "| **Community**      | Used widely in industry                                          | Popular in academia and research labs                               |\n",
        "\n",
        "**When to choose TensorFlow:**\n",
        "\n",
        "* You need production-ready tools, scalability, and optimized deployment pipelines.\n",
        "* You want better cross-platform support (mobile, web).\n",
        "* You plan to use high-level APIs like Keras.\n",
        "\n",
        "*When to choose PyTorch:*\n",
        "\n",
        "* You need fast prototyping, debugging, or you‚Äôre doing research.\n",
        "* You prefer code that looks and feels like standard Python.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X5NoB8UvHJ_X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2: **Describe two use cases for Jupyter Notebooks in AI development.**\n",
        "\n",
        "1. Exploratory Data Analysis (EDA)\n",
        "Jupyter Notebooks are ideal for inspecting datasets, visualizing distributions, cleaning data, and generating quick insights. Analysts and data scientists can:\n",
        "\n",
        "* Plot charts using libraries like Matplotlib or Seaborn.\n",
        "* Test preprocessing steps interactively.\n",
        "* Document the process with markdown alongside the code.\n",
        "\n",
        "**2. Model Development and Experimentation**\n",
        "Jupyter provides an interactive environment to:\n",
        "\n",
        "* Build and test AI models incrementally.\n",
        "* Visualize training metrics like loss and accuracy.\n",
        "* Compare different model architectures and parameters in cells.\n",
        "* Keep experiment logs inline for easy tracking.\n",
        "\n",
        "> Bonus: The mix of code, visuals, and notes makes Jupyter a powerful communication tool for sharing ideas with both technical and non-technical teams.\n"
      ],
      "metadata": {
        "id": "HvuDR9otHAZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3: **How does spaCy enhance NLP tasks compared to basic Python string operations?**\n",
        "\n",
        "\n",
        "| Aspect                             | Basic Python String Ops           | spaCy                                                                        |\n",
        "| ---------------------------------- | --------------------------------- | ---------------------------------------------------------------------------- |\n",
        "| **Tokenization**                   | Manual (`split`, `replace`, etc.) | Built-in robust tokenization that handles punctuation, emojis, abbreviations |\n",
        "| **Part-of-Speech Tagging**         | Not supported                     | Automatic POS tagging out of the box                                         |\n",
        "| **Named Entity Recognition (NER)** | Not available                     | Built-in NER to extract people, brands, dates, etc.                          |\n",
        "| **Dependency Parsing**             | Not possible                      | Analyzes syntactic structure between words                                   |\n",
        "| **Language Models**                | No language awareness             | Uses trained statistical models for contextual understanding                 |\n",
        "| **Efficiency**                     | Slower for complex tasks          | Extremely fast and optimized in Cython                                       |\n",
        "\n",
        "**Summary:**\n",
        "While Python string methods are fine for basic manipulation, spaCy provides deep linguistic processing with minimal code, which is crucial in real-world NLP tasks like chatbots, sentiment analysis, or entity extraction.\n",
        "\n"
      ],
      "metadata": {
        "id": "aWs0ro_wHsWs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparative Analysis: **Scikit-learn vs TensorFlow**\n",
        "\n",
        "| Criteria                      | **Scikit-learn**                                                                                       | **TensorFlow**                                                                           |\n",
        "| ----------------------------- | ------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------- |\n",
        "| **Target Applications**       | Best for classical machine learning: regression, classification, clustering, SVM, decision trees, etc. | Designed for deep learning tasks: neural networks, CNNs, RNNs, image/audio/text modeling |\n",
        "| **Ease of Use for Beginners** | Very beginner-friendly; simple API and quick setup                                                     | Steeper learning curve, especially for custom deep models                                |\n",
        "| **Community Support**         | Strong in academic and business ML communities; lots of tutorials and examples                         | Massive community, backed by Google; rich ecosystem for production and deployment        |\n",
        "\n",
        "**Summary:**\n",
        "\n",
        "* Use **Scikit-learn** when working with structured/tabular data or traditional ML models.\n",
        "* Use **TensorFlow** when building complex neural networks or deploying deep learning models at scale.\n",
        "\n"
      ],
      "metadata": {
        "id": "dHOFuA13IJf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report"
      ],
      "metadata": {
        "id": "vXuGCXrII-qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load iris dataset\n",
        "iris = load_iris()\n",
        "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "df['species'] = iris.target\n",
        "\n",
        "# Map numeric targets to actual species names\n",
        "df['species'] = df['species'].map({i: name for i, name in enumerate(iris.target_names)})\n",
        "\n",
        "# View dataset\n",
        "df.head()"
      ],
      "metadata": {
        "id": "uF8SVRteJAb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# No missing values, so we move on\n",
        "# Encode labels (already encoded, but let's confirm)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "df['species_encoded'] = encoder.fit_transform(df['species'])\n",
        "\n",
        "# Features and labels\n",
        "X = df[iris.feature_names]\n",
        "y = df['species_encoded']"
      ],
      "metadata": {
        "id": "dRUBQgKPJExw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "uOd92fWGJFjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DecisionTreeClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "p8oqDJ1yJIrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred, average='macro'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred, average='macro'))\n",
        "\n",
        "# Detailed report\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=iris.target_names))"
      ],
      "metadata": {
        "id": "sJWJ5AtxJKmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load MNIST data\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize pixel values (0 to 1)\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Reshape for CNN: (samples, 28, 28, 1)\n",
        "x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')  # 10 classes\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train\n",
        "history = model.fit(x_train, y_train, epochs=5, validation_split=0.1)\n",
        "\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(\"Test Accuracy:\", test_acc)\n",
        "\n",
        "\n",
        "# Predict\n",
        "predictions = model.predict(x_test)\n",
        "\n",
        "# Plot 5 sample predictions\n",
        "for i in range(5):\n",
        "    plt.imshow(x_test[i].reshape(28,28), cmap='gray')\n",
        "    plt.title(f\"Predicted: {np.argmax(predictions[i])}, Actual: {y_test[i]}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "SImNfZOWJdkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3: NLP with spaCy ‚Äì Named Entity Recognition (NER) and Sentiment Analysis step by step.\n",
        "\n",
        "#üß† Goal:\n",
        "#Extract product names and brands from user reviews using spaCy's NER\n",
        "\n",
        "#Analyze sentiment (positive/negative) using a rule-based approach\n",
        "\n",
        "#üõ†Ô∏è Step 1: Install and Import spaCy\n",
        "\n",
        "\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "#üì• Step 2: Load spaCy NLP Model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "#üßæ Step 3: Define Sample Amazon Product Reviews\n",
        "reviews = [\n",
        "    \"I love the battery life of the Samsung Galaxy S21. It's so reliable!\",\n",
        "    \"Terrible sound quality on these Sony headphones. Not worth the price.\",\n",
        "    \"Apple iPhone 13 has amazing camera performance. Best purchase ever!\",\n",
        "    \"The Lenovo laptop crashed on the second day. Completely useless!\",\n",
        "    \"Xiaomi smart band is affordable and packed with features. Highly recommend!\"\n",
        "]\n",
        "\n",
        "#üîç Step 4: Named Entity Recognition (NER) to Extract Product Names and Brands\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "\n",
        "print(\"Extracted Entities:\")\n",
        "for review in reviews:\n",
        "    doc = nlp(review)\n",
        "    print(f\"\\nReview: {review}\")\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in [\"ORG\", \"PRODUCT\"]:\n",
        "            print(f\" - Entity: {ent.text} | Label: {ent.label_}\")\n",
        "\n",
        "#üòäüò° Step 5: Rule-Based Sentiment Analysis\n",
        "positive_keywords = ['love', 'amazing', 'reliable', 'best', 'affordable', 'recommend']\n",
        "negative_keywords = ['terrible', 'crashed', 'useless', 'not worth']\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    text_lower = text.lower()\n",
        "    pos = any(word in text_lower for word in positive_keywords)\n",
        "    neg = any(word in text_lower for word in negative_keywords)\n",
        "\n",
        "    if pos and not neg:\n",
        "        return \"Positive\"\n",
        "    elif neg and not pos:\n",
        "        return \"Negative\"\n",
        "    elif pos and neg:\n",
        "        return \"Mixed\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "# Apply to reviews\n",
        "print(\"\\nSentiment Analysis:\")\n",
        "for review in reviews:\n",
        "    sentiment = analyze_sentiment(review)\n",
        "    print(f\" - Review: \\\"{review}\\\" ‚Üí Sentiment: {sentiment}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "khfvf5_eKAjk",
        "outputId": "87b9c372-ca66-4586-9062-d56ca05bfa1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid character 'üß†' (U+1F9E0) (ipython-input-1-1026659899.py, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1-1026659899.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    üß† Goal:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character 'üß†' (U+1F9E0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Perfect. Let‚Äôs tackle **Part 3: Ethics & Optimization** step by step.\n",
        "\n",
        "---\n",
        "\n",
        "# üß≠ *1. Ethical Considerations*\n",
        "\n",
        "# ‚ú® Question:\n",
        "\n",
        "#Identify potential biases in your **MNIST or Amazon Reviews** model. How could tools like **TensorFlow Fairness Indicators** or **spaCy‚Äôs rule-based systems** mitigate these biases?\n",
        "\n",
        "---\n",
        "\n",
        "# ‚úÖ Sample Answer:\n",
        "\n",
        "# > *Bias in Amazon Review NLP model:*\n",
        "\n",
        "# * The sentiment analysis is rule-based and depends on specific keywords.\n",
        "# * This creates a *bias* by:\n",
        "\n",
        "#   * Mislabeling sarcasm or neutral opinions.\n",
        "#   * Not accounting for context or slang.\n",
        "#   * Overrepresenting words like \"love\" or \"terrible\" without understanding nuance.\n",
        "\n",
        "# > *Mitigation with spaCy:*\n",
        "\n",
        "# * Use spaCy's *trained sentiment models* or integrate external *transformer-based models* (like `textblob`, `VADER`, or `RoBERTa`) that account for context.\n",
        "# * Build a *custom rule-based pipeline** in spaCy that detects negations (e.g., \"not bad\" is positive).\n",
        "# * Introduce *user demographic/context data* to spot and reduce biased patterns.\n",
        "\n",
        "# > *Bias in MNIST model:*\n",
        "\n",
        "# * MNIST is often considered neutral, but issues may arise like:\n",
        "\n",
        "#   * The model failing on poorly written digits due to **data imbalance**.\n",
        "#   * Lack of real-world handwriting variance (e.g., cultural styles of writing digits).\n",
        "\n",
        "# > **Mitigation with TensorFlow Fairness Indicators:**\n",
        "\n",
        "# * Integrate *Fairness Indicators* to:\n",
        "\n",
        "#   * Track accuracy across different *subgroups* (e.g., left-handed writers, age groups).\n",
        "#   * Identify if the model favors one writing style over another.\n",
        "#   * Use **reweighting techniques* or *data augmentation* to reduce bias.\n",
        "\n",
        "\n",
        "# üîß *2. Troubleshooting Challenge: Buggy TensorFlow Code*\n",
        "\n",
        "# Here‚Äôs an example of *buggy TensorFlow code* and how to fix it.\n",
        "\n",
        "\n",
        "# ‚ùå Buggy Code (common mistakes)\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define model\n",
        "model = models.Sequential([\n",
        "    layers.Dense(128, input_shape=(28, 28), activation='relu'),  # ERROR: wrong input shape\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])  # ERROR: wrong loss for classification\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#‚úÖ Fixed Code\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Correct input shape and preprocessing\n",
        "model = models.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28)),  # Flatten image input\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Correct loss function for multi-class classification\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Summary\n",
        "model.summary()\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TVlGXPxRKIvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ydRUI1u_LWZp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}